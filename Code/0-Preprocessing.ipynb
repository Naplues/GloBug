{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Step 0 - Preprocessing</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first read the data including the bug reports and source code files of all 51 projects and for ease of access, we save them as two pickle files in the ./Data directory. Therefore, this set of code will populate the ./Data directory with \"allBugReports.pickle\" which is a pandas dataframe that contains all the bug reports from all projects and \"allSourceCodes.pickle\" which is a pandas dataframe that contains all source files after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "from scipy import spatial\n",
    "import scipy.spatial.distance\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_natural_lang(doc):\n",
    "    \"\"\"\n",
    "    @Receives: a document in natural language (bugreport)\n",
    "    @Process: splits it as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    wordList=[]\n",
    "    word=''\n",
    "    for char in doc:\n",
    "        if char.isalnum() or char=='\\'':\n",
    "            word+=char\n",
    "        else:\n",
    "            if len(word)>0:\n",
    "                wordList.append(word)\n",
    "                word=''\n",
    "    if len(word)>0:\n",
    "        wordList.append(word)\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def code_splitter(sourceCode):\n",
    "    \"\"\"\n",
    "    @Receives: a code\n",
    "    @Process: splits it same as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    contentBuf = []\n",
    "    wordBuf = []\n",
    "    for char in sourceCode:\n",
    "        if ((char >= 'a' and char <= 'z') or (char >= 'A' and char <= 'Z')):\n",
    "            wordBuf.append(char)\n",
    "            continue\n",
    "        length = len(wordBuf)\n",
    "        if (length != 0):\n",
    "            k = 0\n",
    "            for i in range(length-1):\n",
    "                j=i+1\n",
    "                first_char = wordBuf[i]\n",
    "                second_char = wordBuf[j]\n",
    "                if ((first_char >= 'A' and first_char <= 'Z') and (second_char >= 'a' and second_char <= 'z')):\n",
    "                    contentBuf.append(wordBuf[k:i])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = i\n",
    "                    continue\n",
    "                if ((first_char >= 'a' and first_char <= 'z') and (second_char >= 'A' and second_char <= 'Z')):\n",
    "                    contentBuf.append(wordBuf[k:j])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = j\n",
    "                    continue\n",
    "            if (k < length):\n",
    "                contentBuf.append(wordBuf[k:])\n",
    "                contentBuf.append(\" \")\n",
    "            wordBuf=[]\n",
    "    words=''\n",
    "    for each in contentBuf:\n",
    "        if isinstance(each,str):\n",
    "            words+=each\n",
    "        else: \n",
    "            for term in each: \n",
    "                words+=term\n",
    "    words= words.split()\n",
    "    contentBuf = []\n",
    "    for i in range(len(words)):\n",
    "        if (words[i].strip()!=\"\" and len(words[i]) >= 2):\n",
    "            contentBuf.append(words[i])\n",
    "    return contentBuf\n",
    "\n",
    "\n",
    "\n",
    "def general_preprocessor(doc,mode):\n",
    "    \"\"\"\n",
    "    @Receives: a document (code or bug report denoted by mode)\n",
    "    @Process: processes the docucument by stemming and removing stop-words and converting to lower cases\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    JavaKeywords=[\"abstract\", \"continue\", \"for\", \n",
    "                \"new\", \"switch\", \"assert\", \"default\", \"goto\", \"package\", \n",
    "                \"synchronized\", \"boolean\", \"do\", \"if\", \"private\", \"this\", \n",
    "                \"break\", \"double\", \"implements\", \"protected\", \"throw\", \"byte\", \n",
    "                \"else\", \"import\", \"public\", \"throws\", \"case\", \"enum\", \n",
    "                \"instanceof\", \"return\", \"transient\", \"catch\", \"extends\", \"int\", \n",
    "                \"short\", \"try\", \"char\", \"final\", \"interface\", \"static\", \"void\", \n",
    "                \"class\", \"finally\", \"long\", \"strictfp\", \"volatile\", \"const\", \n",
    "                \"float\", \"native\", \"super\", \"while\", \"org\", \"eclipse\", \"swt\", \n",
    "                \"string\", \"main\", \"args\", \"null\", \"this\", \"extends\", \"true\", \n",
    "                \"false\"]\n",
    "    stop_words=[\"a\", \"a's\", \"able\", \"about\", \"above\",\n",
    "                \"according\", \"accordingly\", \"across\", \"actually\", \"after\",\n",
    "                \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \"allow\",\n",
    "                \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "                \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\",\n",
    "                \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\",\n",
    "                \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\",\n",
    "                \"appreciate\", \"appropriate\", \"are\", \"aren't\", \"around\", \"as\",\n",
    "                \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\",\n",
    "                \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\",\n",
    "                \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\",\n",
    "                \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "                \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\",\n",
    "                \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \"can't\", \"cannot\",\n",
    "                \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\",\n",
    "                \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\",\n",
    "                \"consequently\", \"consider\", \"considering\", \"contain\",\n",
    "                \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn't\",\n",
    "                \"course\", \"currently\", \"d\", \"definitely\", \"described\",\n",
    "                \"despite\", \"did\", \"didn't\", \"different\", \"do\", \"does\",\n",
    "                \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\",\n",
    "                \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\",\n",
    "                \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\",\n",
    "                \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n",
    "                \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"far\",\n",
    "                \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\",\n",
    "                \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\",\n",
    "                \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\",\n",
    "                \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\",\n",
    "                \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\",\n",
    "                \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\",\n",
    "                \"he's\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\",\n",
    "                \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "                \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\",\n",
    "                \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\",\n",
    "                \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\",\n",
    "                \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\",\n",
    "                \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn't\", \"it\",\n",
    "                \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \"k\",\n",
    "                \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\",\n",
    "                \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\",\n",
    "                \"lest\", \"let\", \"let's\", \"like\", \"liked\", \"likely\", \"little\",\n",
    "                \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\",\n",
    "                \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\",\n",
    "                \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\",\n",
    "                \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\",\n",
    "                \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n",
    "                \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\",\n",
    "                \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\",\n",
    "                \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\", \"oh\",\n",
    "                \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\",\n",
    "                \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\",\n",
    "                \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\",\n",
    "                \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\",\n",
    "                \"placed\", \"please\", \"plus\", \"possible\", \"presumably\",\n",
    "                \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\",\n",
    "                \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\",\n",
    "                \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\",\n",
    "                \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\",\n",
    "                \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\",\n",
    "                \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\",\n",
    "                \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\",\n",
    "                \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\",\n",
    "                \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\",\n",
    "                \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\",\n",
    "                \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\",\n",
    "                \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\",\n",
    "                \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\",\n",
    "                \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\",\n",
    "                \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\",\n",
    "                \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\",\n",
    "                \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\",\n",
    "                \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\",\n",
    "                \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\",\n",
    "                \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\",\n",
    "                \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\",\n",
    "                \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\",\n",
    "                \"viz\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\",\n",
    "                \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\",\n",
    "                \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\",\n",
    "                \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\",\n",
    "                \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n",
    "                \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\",\n",
    "                \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\",\n",
    "                \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\",\n",
    "                \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\",\n",
    "                \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
    "                \"yourself\", \"yourselves\", \"z\", \"zero\",\"quot\"]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    Java_keyWords=[porter.stem(each.strip().lower()) for each in JavaKeywords]\n",
    "    natural_stop_words=[porter.stem(each.strip().lower()) for each in stop_words]\n",
    "    \n",
    "    processed_doc=[]\n",
    "    if mode==\"code\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in code_splitter(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in Java_keyWords or\n",
    "                                                            term in natural_stop_words or len(term)<2)]\n",
    "    elif mode==\"text\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in split_natural_lang(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in natural_stop_words or len(term)<2)]\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading source codes into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "\n",
    "def loadSourceFiles2df(PATH,group,project):\n",
    "    \"\"\"\n",
    "    Receives: group name and project name \n",
    "    Process: open the source file directory and finds all the java files,\n",
    "             and after preprocessing(using code_preprocessor) load them into a pandas dataframe \n",
    "    Returns: dataframe >> \"filename\",\"code\",\"size\"\n",
    "    \"\"\"\n",
    "    print('Loading source files of {} from group:{} ...'.format(project,group))\n",
    "    PATH=os.path.join(\"../Bench4BL/data\",group,project,\"gitrepo\")\n",
    "    all_source_files=glob.glob(PATH+'/**/*.java', recursive=True)\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    sourceCodesList=[]\n",
    "\n",
    "    for filename in tqdm_notebook(all_source_files):\n",
    "        code=open(filename,encoding='ISO-8859-1').read()\n",
    "        processed_code=general_preprocessor(code_parser(code),'code')\n",
    "        if 'src/' in filename:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split('src/')[1].replace('/','.').lower(),\n",
    "                                         \"code\":processed_code,\"unprocessed_code\":code,\n",
    "                                         \"size\":len(processed_code),'project':project}))\n",
    "        else:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split(project)[1].replace('/','.').lower(),\n",
    "                                         \"code\":processed_code,\"unprocessed_code\":code,\n",
    "                                         \"size\":len(processed_code),'project':project}))\n",
    "    source_codes_df=source_codes_df.append(pd.DataFrame(sourceCodesList))\n",
    "    return source_codes_df\n",
    "\n",
    "def load_all_SCs(dataPath):\n",
    "    print('\\tLoading all source codes ... ')\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    all_groups=[folder for folder in listdir(dataPath)]\n",
    "    for group in tqdm_notebook(all_groups):\n",
    "        all_projects= [folder for folder in listdir(os.path.join(dataPath,group))]\n",
    "        for project in all_projects:\n",
    "            source_path=os.path.join(dataPath,group,project,\"gitrepo\")\n",
    "            source_codes_df=source_codes_df.append(loadSourceFiles2df(source_path,group,project))\n",
    "    return source_codes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading bug reports pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBugs2df(PATH,project):\n",
    "    \"\"\"\n",
    "    @Receives: the path to bug repository (the xml file)\n",
    "    @Process: Parses the xml file and reads the fix files per bug id. \n",
    "    @Returns: Returns the dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading Bug reports ... \")\n",
    "    all_bugs_df=pd.DataFrame([],columns=[\"id\",\"fix\",\"text\",\"fixdate\"])\n",
    "    bugRepo = ET.parse(PATH).getroot()\n",
    "    buglist=[]                   \n",
    "    for bug in tqdm_notebook(bugRepo.findall('bug')):\n",
    "        bugDict=dict({\"id\":bug.attrib['id'],\"fix\":[],\"fixdate\":bug.attrib['fixdate']\n",
    "                      ,\"summary\":None,\"description\":None,\"project\":project,\"average_precision\":0.0})\n",
    "        for bugDetail in bug.find('buginformation'):\n",
    "            if bugDetail.tag=='summary':\n",
    "                bugDict[\"summary\"]=bugDetail.text\n",
    "            elif bugDetail.tag=='description':\n",
    "                bugDict[\"description\"]=bugDetail.text\n",
    "        bugDict[\"fix\"]=np.array([fixFile.text.replace('/','.').lower() for fixFile in bug.find('fixedFiles')])\n",
    "        summary=str(bugDict['summary']) if str(bugDict['summary']) !=np.nan else \"\"\n",
    "        description=str(bugDict['description']) if str(bugDict['description']) !=np.nan else \"\"\n",
    "        processed_text=general_preprocessor(summary+\" \"+description,\"text\")\n",
    "        bugDict[\"text\"]=processed_text\n",
    "        buglist.append(bugDict)\n",
    "    all_bugs_df=all_bugs_df.append(pd.DataFrame(buglist))\n",
    "    return all_bugs_df.set_index('id')\n",
    "\n",
    "def load_all_BRs(dataPath):\n",
    "    print('\\tLoading all bug reports ... ')\n",
    "    all_bugs_df=pd.DataFrame([])\n",
    "    all_groups=[folder for folder in listdir(dataPath)]\n",
    "    for group in tqdm_notebook(all_groups):\n",
    "        all_projects= [folder for folder in listdir(os.path.join(dataPath,group))]\n",
    "        for project in all_projects:\n",
    "            data_path=os.path.join(dataPath,group,project,\"bugrepo\",\"repository.xml\")\n",
    "            all_bugs_df=all_bugs_df.append(loadBugs2df(data_path,project))\n",
    "            print(len(all_bugs_df))\n",
    "    return all_bugs_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main Preprocessing class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingUnit:\n",
    "\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    def __init__(self,dataPath):\n",
    "\n",
    "        self.dataPath=dataPath\n",
    "        self.dataFolder=os.path.join(os.getcwd(),'Data')\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "\n",
    "        self.loadEverything()\n",
    "\n",
    "    def loadEverything(self):\n",
    "        vectorize=False\n",
    "        if PreprocessingUnit.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                PreprocessingUnit.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "            else: \n",
    "                PreprocessingUnit.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "        print(\"*** All bug reports are are preprocessed and stored as: {} ***\".format('/'.join(bugReportFile.split('/')[-2:])))\n",
    "\n",
    "        if PreprocessingUnit.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                PreprocessingUnit.all_projects_source_codes=load_all_SCs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "            else:\n",
    "                PreprocessingUnit.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "        print(\"*** All source codes are preprocessed and stored as: {} ***\".format('/'.join(sourceCodeFile.split('/')[-2:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** All bug reports are are preprocessed and stored as: Data/allBugReports.pickle ***\n",
      "*** All source codes are preprocessed and stored as: Data/allSourceCodes.pickle ***\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    config={'DATA_PATH':os.path.join('../Bench4BL','data')}\n",
    "    preprocessor=PreprocessingUnit(dataPath=config['DATA_PATH'])\n",
    "    preprocessor.execute()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
