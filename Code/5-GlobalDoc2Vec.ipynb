{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Global Doc2Vec</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this method, we first create a general global Doc2Vec using all bug reports and all source codes of all projects. the model is expected to learn the common language used in codes and reports and we latter use this globally trained model for connecting new bug reports to the relevant source codes. \n",
    "- Note that TF.IDF scores are calculated using the __global data__. That means that for each term we calculate the term frequency and inverse document frequency of that term using all bug reports and source codes of all projects. \n",
    "- Refer to the third row of the following table (Method5 - Global Doc2Vec) to better understand the details of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim import models\n",
    "\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from time import gmtime, strftime\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Evaluators</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_helper(ranked_files,fixes):\n",
    "    \"\"\"\n",
    "    @Receives: list of ranked files(which is predicted by algorithm) and the ground truth(fixes)\n",
    "    @Process: This is a function aimed to help evaluate, and calculates the AP and first_pos \n",
    "    @Returns: A dictionary containing the AP(average precision) and first_pos (first retrieved fix file)\n",
    "    \"\"\"\n",
    "    found=0\n",
    "    first_pos=-1\n",
    "    average_precision=0\n",
    "    for i,predictionFix in enumerate(ranked_files):\n",
    "        for actualFix in fixes:\n",
    "            if actualFix in predictionFix:\n",
    "                if first_pos==-1:\n",
    "                    first_pos=i+1\n",
    "                found+=1\n",
    "                average_precision+=found/(i+1)        \n",
    "\n",
    "    AP=average_precision/found if found>0 else 0\n",
    "    return {\"AP\":AP,\"first_pos\":first_pos}\n",
    "\n",
    "\n",
    "def evaluate(all_bugs_df,source_codes_df):\n",
    "    \"\"\"\n",
    "    @Receives: The main dataframe\n",
    "    @Process: Evaluates the predicted files for each bugreport in all_bugs_df\n",
    "    @Returns: MAP and MRR calculated from eligible bugreports(the ones with\n",
    "    at least one fix file in this version of code) in the dataframe and number of eligible bug reports.\n",
    "    \"\"\"\n",
    "    all_results=[]\n",
    "    top_founds=[]\n",
    "    average_precisions=[]\n",
    "    for i,br in all_bugs_df.iterrows():\n",
    "        if not source_codes_df.loc[source_codes_df.filename.apply(lambda filename: any(fix in filename for fix in br['fix']))].empty:\n",
    "            predicted_files=br['total_score'].keys()\n",
    "            result=evaluate_helper(predicted_files,br['fix'])\n",
    "            top_founds.append(result['first_pos'])\n",
    "            average_precisions.append(result['AP'])\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            top_founds.append(-1)\n",
    "            average_precisions.append(0.0)\n",
    "    all_bugs_df[\"top_found\"]=top_founds\n",
    "    all_bugs_df[\"average_precision\"]=average_precisions\n",
    "    \n",
    "    #Calculating the MAP and MRR\n",
    "    MAP,MRR=(0,0)\n",
    "    if len(all_results)>0:\n",
    "        for result in all_results:\n",
    "            MAP+=result['AP']\n",
    "            MRR+=1/result['first_pos'] if result['first_pos']>0 else 0\n",
    "        MAP/=len(all_results)\n",
    "        MRR/=len(all_results)\n",
    "        print(\"eligible_br_count: \",len(all_results))\n",
    "    return (MAP,MRR,len(all_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Genral Global Doc2Vec Model </h1></center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_Doc2Vec_models(vec_size,alpha,window_size,all_bugs_df,source_codes_df):\n",
    "    \"\"\"\n",
    "    Process: 1- Loads all the bug reports from all the group/projects in Data directory\n",
    "             2- Makes a Doc2Vec model and trains it based on all the bugreports\n",
    "    Returns: Trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\\t Now building the Combined Doc2Vec model ... \")\n",
    "    dmm_model_path=os.path.join(os.getcwd(),'Models','combined_doc2vec_model_dmm')\n",
    "    dbow_model_path=os.path.join(os.getcwd(),'Models','combined_doc2vec_model_dbow')\n",
    "    fname_dmm = get_tmpfile(dmm_model_path)\n",
    "    fname_dbow = get_tmpfile(dbow_model_path)\n",
    "    if os.path.isfile(dmm_model_path) and os.path.isfile(dbow_model_path):\n",
    "        revectorize=False\n",
    "        model_dmm = Doc2Vec.load(fname_dmm)\n",
    "        model_dbow = Doc2Vec.load(fname_dbow)\n",
    "        print(\"*** Combined Doc2Vec Model is Loaded. ***\")            \n",
    "    else:\n",
    "        revectorize=True\n",
    "        documents = [TaggedDocument(all_bugs_df.iloc[i].text, [i]) for i in range(len(all_bugs_df))]\n",
    "        documents = documents + [TaggedDocument(source_codes_df.iloc[i].code, [len(all_bugs_df)+i]) for i in range(len(source_codes_df))]\n",
    "\n",
    "        model_dmm = Doc2Vec(vector_size=vec_size, window=window_size, min_count=2,\n",
    "                        workers=multiprocessing.cpu_count(),\n",
    "                        alpha=alpha, min_alpha=alpha/2,dm=1)\n",
    "        model_dmm.build_vocab(documents)\n",
    "        model_dmm.train(documents,total_examples=model_dmm.corpus_count,epochs=20)\n",
    "        model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        model_dmm.save(fname_dmm)\n",
    "        \n",
    "        model_dbow = Doc2Vec(dm=0, vector_size=vec_size, negative=5,\n",
    "                             hs=0,min_count=2, sample = 0, workers=multiprocessing.cpu_count(),\n",
    "                             alpha=alpha, min_alpha=alpha/3)\n",
    "        model_dbow.build_vocab(documents)\n",
    "        model_dbow.train(documents,total_examples=model_dbow.corpus_count,epochs=20)\n",
    "        model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        model_dbow.save(fname_dbow)\n",
    "        print(\"*** Combined Doc2Vec Model is Trained. ***\")\n",
    "    concatinated_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    print(\">> Size of Vocabulary is: {}\".format(len(model_dmm.wv.vocab)))\n",
    "    print(\">> Number of whole Documents: {}\".format(model_dmm.corpus_count))\n",
    "    \n",
    "    return (concatinated_model,revectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main BugLocalization class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugLocalizer:\n",
    "\n",
    "    combined_Doc2vec=None\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    vectorize=True\n",
    "    dataFolder=\"\"\n",
    "    \n",
    "    def __init__(self,project,result_path):\n",
    "        self.project=project\n",
    "        self.resultPath=result_path\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "        print(\"\\t ****** Localizing Bugs for project: {} ******\".format(self.project))\n",
    "        revectorize=False\n",
    "        if BugLocalizer.combined_Doc2vec==None:\n",
    "            (BugLocalizer.combined_Doc2vec,revectorize)=build_Doc2Vec_models(vec_size=100,alpha=0.045,window_size=5,\n",
    "                                               all_bugs_df=BugLocalizer.all_projects_bugreports,\n",
    "                                               source_codes_df=BugLocalizer.all_projects_source_codes)\n",
    "            \n",
    "        if BugLocalizer.vectorize:\n",
    "            print('\\n\\t Vectorizing Now ...')\n",
    "            self.vectorizeBugreports()\n",
    "            self.vectorizeSourceCodes()\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            BugLocalizer.all_projects_bugreports.to_pickle(bugReportFile)\n",
    "            BugLocalizer.all_projects_source_codes.to_pickle(sourceCodeFile)\n",
    "            BugLocalizer.vectorize=False\n",
    "            \n",
    "        self.loadBugCurpus()\n",
    "        self.loadSourceFiles()\n",
    "        self.localize()\n",
    "        self.evaluate()\n",
    "        self.to_csv()\n",
    "        self.write_result()\n",
    "        \n",
    "    def loadEverything():\n",
    "        if BugLocalizer.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(BugLocalizer.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                print(\"The bug reports file (allBugReports.pickle) does not exist. please run the step0 first\")\n",
    "            else: \n",
    "                BugLocalizer.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "                print(\"*** All Bug Reports are Loaded. ***\")\n",
    "\n",
    "        if BugLocalizer.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(BugLocalizer.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                print(\"The source codes file (allSourceCodes.pickle) does not exist. please run the step0 first\")\n",
    "            else:\n",
    "                BugLocalizer.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "                print(\"*** All Source Codes are Loaded. ***\")\n",
    "    \n",
    "    def loadBugCurpus(self):\n",
    "        self.all_bugs_df=BugLocalizer.all_projects_bugreports.loc[BugLocalizer.all_projects_bugreports['project']==self.project,:]\n",
    "        self.group=self.all_bugs_df[\"group\"][0]\n",
    "        \n",
    "    def loadSourceFiles(self):\n",
    "        self.source_codes_df=BugLocalizer.all_projects_source_codes.loc[BugLocalizer.all_projects_source_codes['project']==self.project,:]\n",
    "    \n",
    "    def vectorizeBugreports(self):\n",
    "        BugLocalizer.all_projects_bugreports['doc2vec_vector']=np.array(BugLocalizer.all_projects_bugreports.text.apply(BugLocalizer.combined_Doc2vec.infer_vector))\n",
    "\n",
    "    def vectorizeSourceCodes(self):\n",
    "        BugLocalizer.all_projects_source_codes['doc2vec_vector']=np.array(BugLocalizer.all_projects_source_codes.code.apply(BugLocalizer.combined_Doc2vec.infer_vector))\n",
    "     \n",
    "    def localize(self):\n",
    "        \n",
    "        print(\"Localizing Now ...\")\n",
    "        scores=[]\n",
    "        self.source_codes_df=calulateLengthScore(self.source_codes_df)        \n",
    "        doc2vec_index=np.array(list(self.source_codes_df.doc2vec_vector))\n",
    "        for i, br in tqdm_notebook(self.all_bugs_df.iterrows()):\n",
    "            try:\n",
    "                doc2vec_similarities=cos_matrix_multiplication(doc2vec_index, br.doc2vec_vector)\n",
    "                doc2vec_similarities=np_normalizer(doc2vec_similarities)\n",
    "                sourceCodeScores={self.source_codes_df.iloc[j].filename: (doc2vec_similarities[j])*self.source_codes_df.iloc[j].lengthScore \n",
    "                                                  for j in range(len(self.source_codes_df))}\n",
    "                scores.append({file:score for file,score in sorted(sourceCodeScores.items(),key=lambda tup: tup[1],reverse=True)})\n",
    "            except Exception as e:\n",
    "                logging.error(traceback.format_exc())\n",
    "                scores.append({})                \n",
    "        self.all_bugs_df['total_score']=scores\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.result=evaluate(self.all_bugs_df,self.source_codes_df)\n",
    "        print(\"Result/\"+self.group+\"_\"+self.project+\":\\n\\t\",'*'*4,\" MAP: \",self.result[0],'*'*4,'\\n\\t','*'*4,\" MRR: \",self.result[1],'*'*4,\"\\n\",\"-\"*100)\n",
    "\n",
    "    def to_csv(self):\n",
    "        BugReports_path=os.path.join(self.resultPath,'BugReports') \n",
    "        SourceFiles_path=os.path.join(self.resultPath,'SourceFiles')\n",
    "        if not os.path.exists(BugReports_path):\n",
    "            os.makedirs(BugReports_path)\n",
    "        if not os.path.exists(SourceFiles_path):\n",
    "            os.makedirs(SourceFiles_path)\n",
    "        result_Bug_file=os.path.join(BugReports_path,self.project+\"_BugReports.csv\")\n",
    "        result_source_file=os.path.join(SourceFiles_path,self.project+\"_SourceFiles.csv\")\n",
    "        self.all_bugs_df.to_csv(result_Bug_file)\n",
    "        if  len(self.source_codes_df)<100:\n",
    "            self.source_codes_df.to_csv(result_source_file)\n",
    "        \n",
    "    def write_result(self):\n",
    "        group_result=open(os.path.join(self.resultPath,\"results_{}_{}.csv\".format(self.group,self.project)),'w')\n",
    "        group_result.write(group+\" , MAP , MRR , #ofBugReports\\n\")\n",
    "        group_result.write(project+','+str(self.result[0])+','+str(self.result[1])+','+str(self.result[2])+\"\\n\")\n",
    "        group_result.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def folder_structure(run_name):\n",
    "    result_path=os.path.join(os.getcwd(),\"Result\",run_name,strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    return result_path\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    run_name='1-GlobalDoc2Vec'\n",
    "    result_path=folder_structure(run_name)\n",
    "    BugLocalizer.dataFolder=os.path.join(os.getcwd(),'Data')\n",
    "    BugLocalizer.loadEverything()\n",
    "    \n",
    "    all_projects=set(BugLocalizer.all_projects_bugreports.project)\n",
    "    for project in all_projects:\n",
    "        core=BugLocalizer(project=project,result_path=result_path)\n",
    "        core.execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Result</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method='1-GlobalDoc2Vec'\n",
    "runNumber=\"run1\"\n",
    "all_results_csv=[os.path.join(os.getcwd(),\"Result\",method,runNumber,folder) \n",
    "                 for folder in listdir(os.path.join(os.getcwd(),\"Result\",method,runNumber)) if '.csv' in folder]\n",
    "results_df=pd.DataFrame([])\n",
    "for result_csv in all_results_csv:\n",
    "    res=pd.read_csv(result_csv,index_col=[0],header=0)\n",
    "    results_df=results_df.append(res)\n",
    "\n",
    "project_size_df=pd.read_csv('project_size.csv',index_col=[0],header=0)\n",
    "results_df\n",
    "results_df=pd.merge(results_df, project_size_df,\n",
    "                                      left_index=True,\n",
    "                                      right_index=True)\n",
    "results_df=results_df.reset_index()\n",
    "results_df=results_df.set_index(' #ofBugReports')\n",
    "\n",
    "results_df.to_csv(os.path.join(os.getcwd(),\"Result\",method,'result.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
