{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>General Global Doc2Vec</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import operator\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math \n",
    "import time\n",
    "from scipy import spatial\n",
    "import scipy.spatial.distance\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Evaluators</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_helper(ranked_files,fixes):\n",
    "    \"\"\"\n",
    "    @Receives: list of ranked files(which is predicted by algorithm) and the ground truth(fixes)\n",
    "    @Process: This is a function aimed to help evaluate, and calculates the AP and first_pos \n",
    "    @Returns: A dictionary containing the AP(average precision) and first_pos (first retrieved fix file)\n",
    "    \"\"\"\n",
    "    found=0\n",
    "    first_pos=-1\n",
    "    average_precision=0\n",
    "    for i,predictionFix in enumerate(ranked_files):\n",
    "        for actualFix in fixes:\n",
    "            if actualFix.split('.')[-2] == predictionFix:\n",
    "                if first_pos==-1:\n",
    "                    first_pos=i+1\n",
    "                found+=1\n",
    "                average_precision+=found/(i+1)        \n",
    "\n",
    "    AP=average_precision/found if found>0 else 0\n",
    "    return {\"AP\":AP,\"first_pos\":first_pos}\n",
    "\n",
    "\n",
    "def evaluate(all_bugs_df,source_codes_df):\n",
    "    \"\"\"\n",
    "    @Receives: The main dataframe and the path to ClassName.txt \n",
    "    which contains the name of all sourcefiles in this version of code\n",
    "    @Process: Evaluates the predicted files for each bugreport in all_bugs_df\n",
    "    @Returns: MAP and MRR calculated from eligible bugreports(the ones with\n",
    "    at least one fix file in this version of code) in the dataframe and number of eligible bug reports.\n",
    "    \"\"\"\n",
    "    all_results=[]\n",
    "    top_founds=[]\n",
    "    average_precisions=[]\n",
    "    for i,br in all_bugs_df.iterrows():\n",
    "        if not source_codes_df.loc[source_codes_df.filename.apply(lambda filename: any(fix in filename for fix in br['fix']))].empty:\n",
    "            predicted_files=br['total_score'].keys()\n",
    "            result=evaluate_helper(predicted_files,br['fix'])\n",
    "            top_founds.append(result['first_pos'])\n",
    "            average_precisions.append(result['AP'])\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            top_founds.append(-1)\n",
    "            average_precisions.append(0.0)\n",
    "    all_bugs_df[\"top_found\"]=top_founds\n",
    "    all_bugs_df[\"average_precision\"]=average_precisions\n",
    "    \n",
    "    #Calculating the MAP and MRR\n",
    "    MAP,MRR=(0,0)\n",
    "    if len(all_results)>0:\n",
    "        for result in all_results:\n",
    "            MAP+=result['AP']\n",
    "            MRR+=1/result['first_pos'] if result['first_pos']>0 else 0\n",
    "        MAP/=len(all_results)\n",
    "        MRR/=len(all_results)\n",
    "        print(\"eligible_br_count: \",len(all_results))\n",
    "    return (MAP,MRR,len(all_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(doc):\n",
    "    \"\"\"\n",
    "    @Receives: a document\n",
    "    @Process: split it by Capital letters\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    splitted=re.split('\\W+',doc)\n",
    "    return [term.lower() for each in splitted for term in re.sub('(?!^)([A-Z][a-z]+)', r' \\1', each).split()]\n",
    "\n",
    "def split_natural_lang(doc):\n",
    "    \"\"\"\n",
    "    @Receives: a document in natural language\n",
    "    @Process: splits it as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    wordList=[]\n",
    "    word=''\n",
    "    for char in doc:\n",
    "        if char.isalnum() or char=='\\'':\n",
    "            word+=char\n",
    "        else:\n",
    "            if len(word)>0:\n",
    "                wordList.append(word)\n",
    "                word=''\n",
    "    if len(word)>0:\n",
    "        wordList.append(word)\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def code_splitter(sourceCode):\n",
    "\n",
    "    contentBuf = []\n",
    "    wordBuf = []\n",
    "    for char in sourceCode:\n",
    "        if ((char >= 'a' and char <= 'z') or (char >= 'A' and char <= 'Z')):\n",
    "            wordBuf.append(char)\n",
    "            continue\n",
    "        length = len(wordBuf)\n",
    "        if (length != 0):\n",
    "            k = 0\n",
    "            for i in range(length-1):\n",
    "                j=i+1\n",
    "                first_char = wordBuf[i]\n",
    "                second_char = wordBuf[j]\n",
    "                if ((first_char >= 'A' and first_char <= 'Z') and (second_char >= 'a' and second_char <= 'z')):\n",
    "                    contentBuf.append(wordBuf[k:i])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = i\n",
    "                    continue\n",
    "                if ((first_char >= 'a' and first_char <= 'z') and (second_char >= 'A' and second_char <= 'Z')):\n",
    "                    contentBuf.append(wordBuf[k:j])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = j\n",
    "                    continue\n",
    "            if (k < length):\n",
    "                contentBuf.append(wordBuf[k:])\n",
    "                contentBuf.append(\" \")\n",
    "            wordBuf=[]\n",
    "    words=''\n",
    "    for each in contentBuf:\n",
    "        if isinstance(each,str):\n",
    "            words+=each\n",
    "        else: \n",
    "            for term in each: \n",
    "                words+=term\n",
    "    words= words.split()\n",
    "    contentBuf = []\n",
    "    for i in range(len(words)):\n",
    "        if (words[i].strip()!=\"\" and len(words[i]) >= 2):\n",
    "            contentBuf.append(words[i])\n",
    "    return contentBuf\n",
    "\n",
    "def split_code(doc):\n",
    "    \"\"\"\n",
    "    @Receives: a code\n",
    "    @Process: splits it as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    wordList=[]\n",
    "    word=''\n",
    "    for char in doc:\n",
    "        if char.isalpha():\n",
    "            word+=char\n",
    "        else:\n",
    "            if len(word)>0:\n",
    "                wordList+=camel_case_split(word)\n",
    "                word=''\n",
    "    if len(word)>0:\n",
    "        wordList+=camel_case_split(word)\n",
    "    return wordList\n",
    "\n",
    "\n",
    "\n",
    "def general_preprocessor(doc,mode):\n",
    "    JavaKeywords=[\"abstract\", \"continue\", \"for\", \n",
    "                \"new\", \"switch\", \"assert\", \"default\", \"goto\", \"package\", \n",
    "                \"synchronized\", \"boolean\", \"do\", \"if\", \"private\", \"this\", \n",
    "                \"break\", \"double\", \"implements\", \"protected\", \"throw\", \"byte\", \n",
    "                \"else\", \"import\", \"public\", \"throws\", \"case\", \"enum\", \n",
    "                \"instanceof\", \"return\", \"transient\", \"catch\", \"extends\", \"int\", \n",
    "                \"short\", \"try\", \"char\", \"final\", \"interface\", \"static\", \"void\", \n",
    "                \"class\", \"finally\", \"long\", \"strictfp\", \"volatile\", \"const\", \n",
    "                \"float\", \"native\", \"super\", \"while\", \"org\", \"eclipse\", \"swt\", \n",
    "                \"string\", \"main\", \"args\", \"null\", \"this\", \"extends\", \"true\", \n",
    "                \"false\"]\n",
    "    stop_words=[\"a\", \"a's\", \"able\", \"about\", \"above\",\n",
    "                \"according\", \"accordingly\", \"across\", \"actually\", \"after\",\n",
    "                \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \"allow\",\n",
    "                \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "                \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\",\n",
    "                \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\",\n",
    "                \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\",\n",
    "                \"appreciate\", \"appropriate\", \"are\", \"aren't\", \"around\", \"as\",\n",
    "                \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\",\n",
    "                \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\",\n",
    "                \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\",\n",
    "                \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "                \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\",\n",
    "                \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \"can't\", \"cannot\",\n",
    "                \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\",\n",
    "                \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\",\n",
    "                \"consequently\", \"consider\", \"considering\", \"contain\",\n",
    "                \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn't\",\n",
    "                \"course\", \"currently\", \"d\", \"definitely\", \"described\",\n",
    "                \"despite\", \"did\", \"didn't\", \"different\", \"do\", \"does\",\n",
    "                \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\",\n",
    "                \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\",\n",
    "                \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\",\n",
    "                \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n",
    "                \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"far\",\n",
    "                \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\",\n",
    "                \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\",\n",
    "                \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\",\n",
    "                \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\",\n",
    "                \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\",\n",
    "                \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\",\n",
    "                \"he's\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\",\n",
    "                \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "                \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\",\n",
    "                \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\",\n",
    "                \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\",\n",
    "                \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\",\n",
    "                \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn't\", \"it\",\n",
    "                \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \"k\",\n",
    "                \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\",\n",
    "                \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\",\n",
    "                \"lest\", \"let\", \"let's\", \"like\", \"liked\", \"likely\", \"little\",\n",
    "                \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\",\n",
    "                \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\",\n",
    "                \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\",\n",
    "                \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\",\n",
    "                \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n",
    "                \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\",\n",
    "                \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\",\n",
    "                \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\", \"oh\",\n",
    "                \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\",\n",
    "                \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\",\n",
    "                \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\",\n",
    "                \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\",\n",
    "                \"placed\", \"please\", \"plus\", \"possible\", \"presumably\",\n",
    "                \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\",\n",
    "                \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\",\n",
    "                \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\",\n",
    "                \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\",\n",
    "                \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\",\n",
    "                \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\",\n",
    "                \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\",\n",
    "                \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\",\n",
    "                \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\",\n",
    "                \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\",\n",
    "                \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\",\n",
    "                \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\",\n",
    "                \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\",\n",
    "                \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\",\n",
    "                \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\",\n",
    "                \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\",\n",
    "                \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\",\n",
    "                \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\",\n",
    "                \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\",\n",
    "                \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\",\n",
    "                \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\",\n",
    "                \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\",\n",
    "                \"viz\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\",\n",
    "                \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\",\n",
    "                \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\",\n",
    "                \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\",\n",
    "                \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n",
    "                \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\",\n",
    "                \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\",\n",
    "                \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\",\n",
    "                \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\",\n",
    "                \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
    "                \"yourself\", \"yourselves\", \"z\", \"zero\",\"quot\"]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    Java_keyWords=[porter.stem(each.strip().lower()) for each in JavaKeywords]\n",
    "    natural_stop_words=[porter.stem(each.strip().lower()) for each in stop_words]\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    processed_doc=[]\n",
    "    if mode==\"code\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in code_splitter(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in Java_keyWords or\n",
    "                                                            term in natural_stop_words or len(term)<2)]\n",
    "    elif mode==\"text\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in split_natural_lang(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in natural_stop_words or len(term)<2)]\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading Bugreports and Code into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBugs2df(PATH,project):\n",
    "    \"\"\"\n",
    "    @Receives: the path to bug repository (the xml file)\n",
    "    @Process: Parses the xml file and reads the fix files per bug id. \n",
    "    @Returns: Returns the dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading Bug reports ... \")\n",
    "    all_bugs_df=pd.DataFrame([],columns=[\"id\",\"fix\",\"text\",\"fixdate\"])\n",
    "    bugRepo = ET.parse(PATH).getroot()\n",
    "    buglist=[]                   \n",
    "    for bug in tqdm_notebook(bugRepo.findall('bug')):\n",
    "        bugDict=dict({\"id\":bug.attrib['id'],\"fix\":[],\"fixdate\":bug.attrib['fixdate']\n",
    "                      ,\"summary\":None,\"description\":None,\"project\":project,\"average_precision\":0.0})\n",
    "        for bugDetail in bug.find('buginformation'):\n",
    "            if bugDetail.tag=='summary':\n",
    "                bugDict[\"summary\"]=bugDetail.text\n",
    "            elif bugDetail.tag=='description':\n",
    "                bugDict[\"description\"]=bugDetail.text\n",
    "        bugDict[\"fix\"]=np.array([fixFile.text.replace('/','.').lower() for fixFile in bug.find('fixedFiles')])\n",
    "        summary=str(bugDict['summary']) if str(bugDict['summary']) !=np.nan else \"\"\n",
    "        description=str(bugDict['description']) if str(bugDict['description']) !=np.nan else \"\"\n",
    "        processed_text=general_preprocessor(summary+\" \"+description,\"text\")\n",
    "        bugDict[\"text\"]=processed_text\n",
    "        buglist.append(bugDict)\n",
    "    all_bugs_df=all_bugs_df.append(pd.DataFrame(buglist))\n",
    "    return all_bugs_df.set_index('id')\n",
    "\n",
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "\n",
    "def loadSourceFiles2df(PATH,group,project):\n",
    "    \"\"\"\n",
    "    Receives: group name and project name \n",
    "    Process: open the source file directory and finds all the java files,\n",
    "             and after preprocessing(using code_preprocessor) load them into a pandas dataframe \n",
    "    Returns: dataframe >> \"filename\",\"code\",\"size\"\n",
    "    \"\"\"\n",
    "    print('Loading source files of {} from group:{} ...'.format(project,group))\n",
    "    PATH=os.path.join(\"../Bench4BL/data\",group,project,\"gitrepo\")\n",
    "    all_source_files=glob.glob(PATH+'/**/*.java', recursive=True)\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    sourceCodesList=[]\n",
    "\n",
    "    for filename in tqdm_notebook(all_source_files):\n",
    "        code=open(filename,encoding='ISO-8859-1').read()\n",
    "        processed_code=general_preprocessor(code_parser(code),'code')\n",
    "        if 'src/' in filename:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split('src/')[1].replace('/','.').lower(),\n",
    "                                         \"code\":processed_code,\"unprocessed_code\":code,\n",
    "                                         \"size\":len(processed_code),'project':project}))\n",
    "        else:\n",
    "            sourceCodesList.append(dict({\"filename\":filename.split(project)[1].replace('/','.').lower(),\n",
    "                                         \"code\":processed_code,\"unprocessed_code\":code,\n",
    "                                         \"size\":len(processed_code),'project':project}))\n",
    "    source_codes_df=source_codes_df.append(pd.DataFrame(sourceCodesList))\n",
    "    return source_codes_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>TFIDF</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormValue(x,maximum,minimum):\n",
    "    return 6*((x - minimum)/(maximum - minimum))\n",
    "\n",
    "def getLenScore(length):\n",
    "    return (math.exp(length) / (1 + math.exp(length)))\n",
    "\n",
    "def calulateLengthScore(source_codes_df):\n",
    "    \"\"\"\n",
    "    Receives: a list of sizes of codes and the index\n",
    "    Process: calculate a boost score for the specified index based on length of that code\n",
    "    Returns: length boosting score \n",
    "    \"\"\"\n",
    "    average_size=source_codes_df['size'].mean()\n",
    "    standard_deviation=source_codes_df['size'].std() \n",
    "    low=average_size-3*standard_deviation\n",
    "    high= average_size+3*standard_deviation\n",
    "    minimum=int(low) if low>0 else 0\n",
    "        \n",
    "    len_scores=[]\n",
    "    for i,eachLen in source_codes_df['size'].items():\n",
    "        score=0\n",
    "        nor=getNormValue(eachLen,high,minimum)\n",
    "        if eachLen!=0:\n",
    "            if eachLen>=low and eachLen<=high:\n",
    "                score=getLenScore(nor)\n",
    "            elif eachLen<low:\n",
    "                score=0.5\n",
    "            elif eachLen>high:\n",
    "                score = 1.0\n",
    "        len_scores.append(score)\n",
    "    source_codes_df['lengthScore']=len_scores\n",
    "\n",
    "    return source_codes_df\n",
    "    \n",
    "def inverse_doc_freq(idf,D):\n",
    "    return math.log(D/idf)\n",
    "\n",
    "def term_freq(tf_list):\n",
    "    return [(math.log(tf+1)) for tf in tf_list]\n",
    "\n",
    "def np_normalizer(arr):\n",
    "    \"\"\"\n",
    "    @Receives: a list of numbers\n",
    "    @Process: normalizes all the values and map them to range of [0,1]\n",
    "    @Returns: list of normalized numbers\n",
    "    \"\"\"\n",
    "    if len(arr)>0:\n",
    "        maximum=np.amax(arr)\n",
    "        minimum=np.amin(arr)\n",
    "        if maximum!=minimum:\n",
    "            return (arr-minimum)/(maximum-minimum)\n",
    "    return arr\n",
    "\n",
    "def normalizer(Dict):\n",
    "    \"\"\"\n",
    "    @Receives: a list of numbers\n",
    "    @Process: normalizes all the values and map them to range of [0,1]\n",
    "    @Returns: list of normalized numbers\n",
    "    \"\"\"\n",
    "    if len(Dict)>0:\n",
    "        maximum=max(Dict.items(), key=operator.itemgetter(1))[1]\n",
    "        minimum=min(Dict.items(), key=operator.itemgetter(1))[1]\n",
    "        for key,value in Dict.items():\n",
    "            if maximum!=minimum:\n",
    "                Dict[key]=(value-minimum)/(maximum-minimum)\n",
    "            else:\n",
    "                Dict[key]=1.0\n",
    "            \n",
    "    return Dict\n",
    "    \n",
    "\n",
    "def TFIDF_transform(all_bugs_df,source_codes_df):\n",
    "    \n",
    "    print(\"\\tTransforming to TF.IDF ...\")\n",
    "    dictionary = gensim.corpora.Dictionary(list(source_codes_df['code']))\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in list(source_codes_df['code'])]\n",
    "    tfidf_weights = models.TfidfModel(corpus,wlocal=term_freq,wglobal=inverse_doc_freq,normalize=False)\n",
    "    source_codes_df['tfidf_vector']=tfidf_weights[corpus]\n",
    "    all_bugs_df['tfidf_vector']=all_bugs_df.text.apply(lambda x: tfidf_weights[dictionary.doc2bow(x)])\n",
    "    return (all_bugs_df,source_codes_df,len(dictionary))\n",
    "\n",
    "def cos_matrix_multiplication(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using matrix vector multiplication.\n",
    "    \"\"\"\n",
    "    dotted = matrix.dot(vector)\n",
    "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Genral Global Doc2Vec Model </h1></center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_BRs(dataPath):\n",
    "    print('\\tLoading all bug reports ... ')\n",
    "    all_bugs_df=pd.DataFrame([])\n",
    "    all_groups=[folder for folder in listdir(dataPath)]\n",
    "    for group in tqdm_notebook(all_groups):\n",
    "        all_projects= [folder for folder in listdir(os.path.join(dataPath,group))]\n",
    "        for project in all_projects:\n",
    "            data_path=os.path.join(dataPath,group,project,\"bugrepo\",\"repository.xml\")\n",
    "            all_bugs_df=all_bugs_df.append(loadBugs2df(data_path,project))\n",
    "            print(len(all_bugs_df))\n",
    "    return all_bugs_df\n",
    "\n",
    "def load_all_SCs(dataPath):\n",
    "    print('\\tLoading all source codes ... ')\n",
    "    source_codes_df=pd.DataFrame([])\n",
    "    all_groups=[folder for folder in listdir(dataPath)]\n",
    "    for group in tqdm_notebook(all_groups):\n",
    "        all_projects= [folder for folder in listdir(os.path.join(dataPath,group))]\n",
    "        for project in all_projects:\n",
    "            source_path=os.path.join(dataPath,group,project,\"gitrepo\")\n",
    "            source_codes_df=source_codes_df.append(loadSourceFiles2df(source_path,group,project))\n",
    "    return source_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Doc2Vec_models(vec_size,alpha,window_size,all_bugs_df,source_codes_df):\n",
    "    \"\"\"\n",
    "    Process: 1- Loads all the bug reports from all the group/projects in Data directory\n",
    "             2- Makes a Doc2Vec model and trains it based on all the bugreports\n",
    "    Returns: Trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\\t Now building the Combined Doc2Vec model ... \")\n",
    "    dmm_model_path=os.path.join(os.getcwd(),'Models','combined_doc2vec_model_dmm')\n",
    "    dbow_model_path=os.path.join(os.getcwd(),'Models','combined_doc2vec_model_dbow')\n",
    "    fname_dmm = get_tmpfile(dmm_model_path)\n",
    "    fname_dbow = get_tmpfile(dbow_model_path)\n",
    "    if os.path.isfile(dmm_model_path) and os.path.isfile(dbow_model_path):\n",
    "        revectorize=False\n",
    "        model_dmm = Doc2Vec.load(fname_dmm)\n",
    "        model_dbow = Doc2Vec.load(fname_dbow)\n",
    "        print(\"*** Combined Doc2Vec Model is Loaded. ***\")            \n",
    "    else:\n",
    "        revectorize=True\n",
    "        documents = [TaggedDocument(all_bugs_df.iloc[i].text, [i]) for i in range(len(all_bugs_df))]\n",
    "        documents = documents + [TaggedDocument(source_codes_df.iloc[i].code, [len(all_bugs_df)+i]) for i in range(len(source_codes_df))]\n",
    "\n",
    "        model_dmm = Doc2Vec(vector_size=vec_size, window=window_size, min_count=2,\n",
    "                        workers=multiprocessing.cpu_count(),\n",
    "                        alpha=alpha, min_alpha=alpha/2,dm=1)\n",
    "        model_dmm.build_vocab(documents)\n",
    "        model_dmm.train(documents,total_examples=model_dmm.corpus_count,epochs=20)\n",
    "        model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        model_dmm.save(fname_dmm)\n",
    "        \n",
    "        model_dbow = Doc2Vec(dm=0, vector_size=vec_size, negative=5,\n",
    "                             hs=0,min_count=2, sample = 0, workers=multiprocessing.cpu_count(),\n",
    "                             alpha=alpha, min_alpha=alpha/3)\n",
    "        model_dbow.build_vocab(documents)\n",
    "        model_dbow.train(documents,total_examples=model_dbow.corpus_count,epochs=20)\n",
    "        model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        model_dbow.save(fname_dbow)\n",
    "        print(\"*** Combined Doc2Vec Model is Trained. ***\")\n",
    "    concatinated_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    print(\">> Size of Vocabulary is: {}\".format(len(model_dmm.wv.vocab)))\n",
    "    print(\">> Number of whole Documents: {}\".format(model_dmm.corpus_count))\n",
    "    \n",
    "    return (concatinated_model,revectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(sourceCodeScores,bugReportScores):\n",
    "    sourceCodeScores=normalizer(sourceCodeScores)\n",
    "    bugReportScores=normalizer(bugReportScores)\n",
    "    for file in bugReportScores.keys():\n",
    "        if file in sourceCodeScores.keys():\n",
    "            sourceCodeScores[file]=sourceCodeScores[file]*0.8+bugReportScores[file]*0.2\n",
    "    return sourceCodeScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main BugLocalization class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugLocalizer:\n",
    "    \n",
    "    TFIDF_transformed=False\n",
    "    dictionary_length=0\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    def __init__(self,group,project,result_path,dataPath):\n",
    "        self.group=group\n",
    "        self.project=project\n",
    "        self.resultPath=result_path\n",
    "        self.dataPath=dataPath\n",
    "        self.dataFolder=os.path.join(os.getcwd(),'Data')\n",
    "        if not os.path.exists(self.dataFolder):\n",
    "            os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "        print(\"\\t ****** Localizing Bugs for group: {} , project: {} ******\".format(self.group,self.project))\n",
    "        vectorize=self.loadEverything()\n",
    "        revectorize=False\n",
    "        \n",
    "        if not BugLocalizer.TFIDF_transformed:\n",
    "            (self.all_projects_bugreports,BugLocalizer.all_projects_source_codes,BugLocalizer.dictionary_length)=TFIDF_transform(all_bugs_df=BugLocalizer.all_projects_bugreports,\n",
    "                                                                                                     source_codes_df=BugLocalizer.all_projects_source_codes)                                       \n",
    "            BugLocalizer.TFIDF_transformed=True\n",
    "\n",
    "        self.loadBugCurpus()\n",
    "        self.loadSourceFiles()\n",
    "        self.localize()\n",
    "        self.evaluate()\n",
    "        self.to_csv()\n",
    "        self.write_result()\n",
    "        \n",
    "    def loadEverything(self):\n",
    "        vectorize=False\n",
    "        if BugLocalizer.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(self.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                BugLocalizer.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "            else: \n",
    "                BugLocalizer.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "                print(\"*** All Bug Reports are Loaded. ***\")\n",
    "\n",
    "        if BugLocalizer.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(self.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                BugLocalizer.all_projects_source_codes=load_all_SCs(dataPath=self.dataPath)\n",
    "                vectorize=True\n",
    "            else:\n",
    "                BugLocalizer.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "                print(\"*** All Source Codes are Loaded. ***\")\n",
    "        return vectorize\n",
    "    \n",
    "    def loadBugCurpus(self):\n",
    "        self.all_bugs_df=BugLocalizer.all_projects_bugreports.loc[BugLocalizer.all_projects_bugreports['project']==self.project,:]\n",
    "    \n",
    "    def loadSourceFiles(self):\n",
    "        self.source_codes_df=BugLocalizer.all_projects_source_codes.loc[BugLocalizer.all_projects_source_codes['project']==self.project,:]\n",
    "    \n",
    "    def vectorizeBugreports(self):\n",
    "        BugLocalizer.all_projects_bugreports['doc2vec_vector']=np.array(BugLocalizer.all_projects_bugreports.text.apply(BugLocalizer.combined_Doc2vec.infer_vector))\n",
    "        \n",
    "    def vectorizeSourceCodes(self):\n",
    "        BugLocalizer.all_projects_source_codes['doc2vec_vector']=np.array(BugLocalizer.all_projects_source_codes.code.apply(BugLocalizer.combined_Doc2vec.infer_vector))\n",
    "     \n",
    "\n",
    "    def localize(self):\n",
    "        \n",
    "        print(\"Localizing Now ...\")\n",
    "        self.source_codes_df=calulateLengthScore(self.source_codes_df)\n",
    "        scores=[]\n",
    "        direct_tfidf_index = similarities.SparseMatrixSimilarity(list(self.source_codes_df.tfidf_vector),num_features=BugLocalizer.dictionary_length)\n",
    "        indirect_tfidf_index = similarities.SparseMatrixSimilarity(list(self.all_bugs_df.tfidf_vector),num_features=BugLocalizer.dictionary_length)\n",
    "        indirectScores=[]\n",
    "        BRRanks=[]\n",
    "        for i, br in tqdm_notebook(self.all_bugs_df.iterrows()):\n",
    "            \n",
    "            try:\n",
    "                direct_tfidf_similarities=direct_tfidf_index[br.tfidf_vector]\n",
    "                direct_tfidf_similarities=np_normalizer(direct_tfidf_similarities)\n",
    "                sourceCodeScores={self.source_codes_df.iloc[j].filename.split('.')[-2]: (direct_tfidf_similarities[j])*self.source_codes_df.iloc[j].lengthScore \n",
    "                                                  for j in range(len(self.source_codes_df))\n",
    "                                                  if len(self.source_codes_df.iloc[j].filename.split('.'))>1}\n",
    "                \n",
    "                indirect_tfidf_similarities=indirect_tfidf_index[br.tfidf_vector]                \n",
    "                bugReportScores=dict({})\n",
    "                for j,(idx,other_br) in enumerate(self.all_bugs_df.iterrows()):\n",
    "                    for fixFile in other_br.fix:\n",
    "                        if idx != i:\n",
    "                            if fixFile.split('.')[-2] in bugReportScores.keys():\n",
    "                                if indirect_tfidf_similarities[j]>=bugReportScores[fixFile.split('.')[-2]]:\n",
    "                                    bugReportScores[fixFile.split('.')[-2]]=indirect_tfidf_similarities[j]\n",
    "                            else:\n",
    "                                bugReportScores[fixFile.split('.')[-2]]=indirect_tfidf_similarities[j]\n",
    "                indirectScores.append(bugReportScores)\n",
    "                BRRanks.append({idx:indirect_tfidf_similarities[j] for j,(idx,other_br) in enumerate(self.all_bugs_df.iterrows())})\n",
    "                ranking=synthesize(sourceCodeScores,bugReportScores)\n",
    "                scores.append({file:score for file,score in sorted(ranking.items(),key=lambda tup: tup[1],reverse=True)})\n",
    "            except Exception as e:\n",
    "                logging.error(traceback.format_exc())\n",
    "                scores.append({})\n",
    "        self.all_bugs_df['Indirect_scores']=indirectScores\n",
    "        self.all_bugs_df['BR_scores']=BRRanks\n",
    "        self.all_bugs_df['total_score']=scores\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.result=evaluate(self.all_bugs_df,self.source_codes_df)\n",
    "        print(\"Result/\"+self.group+\"_\"+self.project+\":\\n\\t\",'*'*4,\" MAP: \",self.result[0],'*'*4,'\\n\\t','*'*4,\" MRR: \",self.result[1],'*'*4,\"\\n\",\"-\"*100)\n",
    "\n",
    "    def to_csv(self):\n",
    "        BugReports_path=os.path.join(self.resultPath,'BugReports') \n",
    "        SourceFiles_path=os.path.join(self.resultPath,'SourceFiles')\n",
    "        if not os.path.exists(BugReports_path):\n",
    "            os.makedirs(BugReports_path)\n",
    "        if not os.path.exists(SourceFiles_path):\n",
    "            os.makedirs(SourceFiles_path)\n",
    "        result_Bug_file=os.path.join(BugReports_path,self.project+\"_BugReports.csv\")\n",
    "        result_source_file=os.path.join(SourceFiles_path,self.project+\"_SourceFiles.csv\")\n",
    "        self.all_bugs_df.to_csv(result_Bug_file)\n",
    "        if len(self.source_codes_df)<300:\n",
    "            self.source_codes_df.to_csv(result_source_file)\n",
    "        \n",
    "    def write_result(self):\n",
    "        group_result=open(os.path.join(self.resultPath,\"results_{}_{}.csv\".format(self.group,self.project)),'w')\n",
    "        group_result.write(group+\" , MAP , MRR , #ofBugReports\\n\")\n",
    "        group_result.write(project+','+str(self.result[0])+','+str(self.result[1])+','+str(self.result[2])+\"\\n\")\n",
    "        group_result.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ****** Localizing Bugs for group: Spring , project: DATAREST ******\n",
      "*** All Bug Reports are Loaded. ***\n",
      "*** All Source Codes are Loaded. ***\n",
      "\tTransforming to TF.IDF ...\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdaf7f04b0d4173aae88439b7559566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  121\n",
      "Result/Spring_DATAREST:\n",
      "\t ****  MAP:  0.4490201506512615 **** \n",
      "\t ****  MRR:  0.6522439736024456 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SECOAUTH ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3aa2fec69482a81b90c559ed2a8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  66\n",
      "Result/Spring_SECOAUTH:\n",
      "\t ****  MAP:  0.4224606945689348 **** \n",
      "\t ****  MRR:  0.5125508176380477 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: ROO ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dd3f1fcc4d46f2a3a8ad04c389f691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  558\n",
      "Result/Spring_ROO:\n",
      "\t ****  MAP:  0.35704445449433353 **** \n",
      "\t ****  MRR:  0.458686195544525 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: DATAGRAPH ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51404a5ece084f01a98fc8c04fdcddcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  12\n",
      "Result/Spring_DATAGRAPH:\n",
      "\t ****  MAP:  0.1330306942586024 **** \n",
      "\t ****  MRR:  0.19904055941998589 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SHL ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc74c1bf68884029a739017150d05544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  10\n",
      "Result/Spring_SHL:\n",
      "\t ****  MAP:  0.336721955491016 **** \n",
      "\t ****  MRR:  0.40970892839927836 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SOCIALLI ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2a1307241f4769b9bbd16432678181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  4\n",
      "Result/Spring_SOCIALLI:\n",
      "\t ****  MAP:  0.4609445701357466 **** \n",
      "\t ****  MRR:  0.7083333333333333 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SGF ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff4de9266c342959b4d574b733a5aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  98\n",
      "Result/Spring_SGF:\n",
      "\t ****  MAP:  0.39043793555768747 **** \n",
      "\t ****  MRR:  0.637547546406265 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SEC ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea9f013e97c4a5f86da59da4d349f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  362\n",
      "Result/Spring_SEC:\n",
      "\t ****  MAP:  0.5050957635861384 **** \n",
      "\t ****  MRR:  0.6268171765666212 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: LDAP ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d80fa90eeeb4644b8b6a7ca3f9cfb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  52\n",
      "Result/Spring_LDAP:\n",
      "\t ****  MAP:  0.40048466252502823 **** \n",
      "\t ****  MRR:  0.4999351799516273 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: DATAREDIS ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a140d2c7d7349698ad9cf287e500402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  49\n",
      "Result/Spring_DATAREDIS:\n",
      "\t ****  MAP:  0.5308210096719018 **** \n",
      "\t ****  MRR:  0.7435523660013456 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: AMQP ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ddc7a0213f4ca1a6c6f9d8a4716144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  97\n",
      "Result/Spring_AMQP:\n",
      "\t ****  MAP:  0.3838264704708613 **** \n",
      "\t ****  MRR:  0.5383982058312548 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SOCIAL ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e086be016f41d88df882908e5b0a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  13\n",
      "Result/Spring_SOCIAL:\n",
      "\t ****  MAP:  0.5684048476942464 **** \n",
      "\t ****  MRR:  0.5869433198380567 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SPR ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcc302e4a354abc95b9d54718b09f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  123\n",
      "Result/Spring_SPR:\n",
      "\t ****  MAP:  0.28079401649646407 **** \n",
      "\t ****  MRR:  0.3717372366713278 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SOCIALTW ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769f91ba52084faaa4f975282a646aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  8\n",
      "Result/Spring_SOCIALTW:\n",
      "\t ****  MAP:  0.7854166666666667 **** \n",
      "\t ****  MRR:  0.8375 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: DATACMNS ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2369c0004d469fa4feab0ba789bbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  152\n",
      "Result/Spring_DATACMNS:\n",
      "\t ****  MAP:  0.5377867771288761 **** \n",
      "\t ****  MRR:  0.6943900593827476 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: BATCHADM ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea681222d33746f49f3c045e223ed28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  20\n",
      "Result/Spring_BATCHADM:\n",
      "\t ****  MAP:  0.43024239385057966 **** \n",
      "\t ****  MRR:  0.559749968862872 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: MOBILE ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8916aef0ea8740ad8ddef69ea4db3c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  11\n",
      "Result/Spring_MOBILE:\n",
      "\t ****  MAP:  0.6524257125057903 **** \n",
      "\t ****  MRR:  0.8409090909090909 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: BATCH ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2e8394a48d40958c6f93e2edb20c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  354\n",
      "Result/Spring_BATCH:\n",
      "\t ****  MAP:  0.407679293770265 **** \n",
      "\t ****  MRR:  0.5850184952462338 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SHDP ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013f67ade7b94404b856925a47cbc1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  45\n",
      "Result/Spring_SHDP:\n",
      "\t ****  MAP:  0.44097836022922016 **** \n",
      "\t ****  MRR:  0.5880282626734297 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: ANDROID ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74b58691b6e4a2c9e1b945389688b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  9\n",
      "Result/Spring_ANDROID:\n",
      "\t ****  MAP:  0.3276150336084019 **** \n",
      "\t ****  MRR:  0.6462962962962963 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SOCIALFB ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f6ba80b8724764be176564a4331133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  15\n",
      "Result/Spring_SOCIALFB:\n",
      "\t ****  MAP:  0.5561141986283488 **** \n",
      "\t ****  MRR:  0.648994708994709 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: DATAMONGO ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e807fbf138b42569570f29541eab187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  264\n",
      "Result/Spring_DATAMONGO:\n",
      "\t ****  MAP:  0.3601272880644268 **** \n",
      "\t ****  MRR:  0.5138057186752736 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SWF ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4a3ede01954a059ea07bed4e4ebf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  105\n",
      "Result/Spring_SWF:\n",
      "\t ****  MAP:  0.45044285334664264 **** \n",
      "\t ****  MRR:  0.5490116009868069 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: SWS ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f7c9e2711948d9939271669938ad6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  159\n",
      "Result/Spring_SWS:\n",
      "\t ****  MAP:  0.42290301283378834 **** \n",
      "\t ****  MRR:  0.5403831617190844 **** \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "\t ****** Localizing Bugs for group: Spring , project: DATAJPA ******\n",
      "Localizing Now ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af8eddfcd074deab6780ef54199b0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eligible_br_count:  144\n",
      "Result/Spring_DATAJPA:\n",
      "\t ****  MAP:  0.48877886650884034 **** \n",
      "\t ****  MRR:  0.6813327250867564 **** \n",
      " ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def folder_structure(run_name):\n",
    "    result_path=os.path.join(os.getcwd(),\"Result\",run_name,strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    return result_path\n",
    "    \n",
    "def get_all_groups(config):\n",
    "    return [folder for folder in listdir(config['DATA_PATH'])] if config['specefic_groups']==[] else config['specefic_groups']\n",
    "\n",
    "def get_all_projects(config,group):\n",
    "    return [folder for folder in listdir(os.path.join(config['DATA_PATH'],group))] if config['specefic_projects']==[] else config['specefic_projects']\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    config={'specefic_groups':['Spring'],'specefic_projects':[],'DATA_PATH':os.path.join('../Bench4BL','data')}\n",
    "    run_name='7-GlobalBugLocator'\n",
    "    result_path=folder_structure(run_name)\n",
    "    for group in get_all_groups(config):\n",
    "        for project in get_all_projects(config,group):\n",
    "            core=BugLocalizer(group=group,project=project,\n",
    "                              result_path=result_path,dataPath=config['DATA_PATH'])\n",
    "            core.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method='7-GlobalBugLocator'\n",
    "runNumber=\"run\"\n",
    "all_results_csv=[os.path.join(os.getcwd(),\"Result\",method,runNumber,folder) \n",
    "                 for folder in listdir(os.path.join(os.getcwd(),\"Result\",method,runNumber)) if '.csv' in folder]\n",
    "results_df=pd.DataFrame([])\n",
    "for result_csv in all_results_csv:\n",
    "    res=pd.read_csv(result_csv,index_col=[0],header=0)\n",
    "    results_df=results_df.append(res)\n",
    "\n",
    "project_size_df=pd.read_csv('project_size.csv',index_col=[0],header=0)\n",
    "results_df\n",
    "results_df=pd.merge(results_df, project_size_df,\n",
    "                                      left_index=True,\n",
    "                                      right_index=True)\n",
    "results_df=results_df.reset_index()\n",
    "results_df=results_df.set_index(' #ofBugReports')\n",
    "\n",
    "results_df.to_csv(os.path.join(os.getcwd(),\"Result\",method,'result.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Testing</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from gensim.models.deprecated import keyedvectors\n",
    "from gensim.models.deprecated.keyedvectors import EuclideanKeyedVectors\n",
    "import numpy as np\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# print(len(BugLocalizer.all_projects_source_codes.loc[BugLocalizer.all_projects_source_codes['project']=='IO']))\n",
    "\n",
    "vectors_all=np.array(BugLocalizer.all_projects_source_codes.iloc[:20].doc2vec_vector)\n",
    "vector_1=BugLocalizer.all_projects_bugreports.iloc[0].doc2vec_vector\n",
    "print(vector_1.shape,vectors_all.shape)\n",
    "EuclideanKeyedVectors.cosine_similarities(vector_1=vector_1,vectors_all=vectors_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCodes_df=pd.DataFrame(BugLocalizer.all_projects_source_codes.groupby('project').size())\n",
    "sourceCodes_df.columns=[\"SourceCodeSize\"]\n",
    "sourceCodes_df.index.name='Project'\n",
    "sourceCodes_df.to_csv('project_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
