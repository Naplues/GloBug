{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Method 1: Local TFIDF</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this method, we perform the localization __only directly__ using the the TFIDF similarity score of bug reports to source codes in each project individually. \n",
    "- Note that TF.IDF scores are calculated using the __local data__. That means that for each term we calculate the term frequency and inverse document frequency of that term using only the bug reports and source codes of that project. \n",
    "- Refer to the following table to better understand the details of this method.\n",
    "\n",
    "<img src=\"Methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import operator\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "\n",
    "import warnings\n",
    "import math\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from time import gmtime, strftime\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Evaluators</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_helper(ranked_files,fixes):\n",
    "    \"\"\"\n",
    "    @Receives: list of ranked files(which is predicted by algorithm) and the ground truth(fixes)\n",
    "    @Process: This is a function aimed to help evaluate, and calculates the AP and first_pos \n",
    "    @Returns: A dictionary containing the AP(average precision) and first_pos (first retrieved fix file)\n",
    "    \"\"\"\n",
    "    found=0\n",
    "    first_pos=-1\n",
    "    average_precision=0\n",
    "    for i,predictionFix in enumerate(ranked_files):\n",
    "        for actualFix in fixes:\n",
    "            if actualFix in predictionFix:\n",
    "                if first_pos==-1:\n",
    "                    first_pos=i+1\n",
    "                found+=1\n",
    "                average_precision+=found/(i+1)        \n",
    "\n",
    "    AP=average_precision/found if found>0 else 0\n",
    "    return {\"AP\":AP,\"first_pos\":first_pos}\n",
    "\n",
    "\n",
    "def evaluate(all_bugs_df,source_codes_df):\n",
    "    \"\"\"\n",
    "    @Receives: The main dataframe\n",
    "    @Process: Evaluates the predicted files for each bugreport in all_bugs_df\n",
    "    @Returns: MAP and MRR calculated from eligible bugreports(the ones with\n",
    "    at least one fix file in this version of code) in the dataframe and number of eligible bug reports.\n",
    "    \"\"\"\n",
    "    all_results=[]\n",
    "    top_founds=[]\n",
    "    average_precisions=[]\n",
    "    for i,br in all_bugs_df.iterrows():\n",
    "        if not source_codes_df.loc[source_codes_df.filename.apply(lambda filename: any(fix in filename for fix in br['fix']))].empty:\n",
    "            predicted_files=br['total_score'].keys()\n",
    "            result=evaluate_helper(predicted_files,br['fix'])\n",
    "            top_founds.append(result['first_pos'])\n",
    "            average_precisions.append(result['AP'])\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            top_founds.append(-1)\n",
    "            average_precisions.append(0.0)\n",
    "    all_bugs_df.loc[:,\"top_found\"]=top_founds\n",
    "    all_bugs_df.loc[:,\"average_precision\"]=average_precisions\n",
    "    \n",
    "    #Calculating the MAP and MRR\n",
    "    MAP,MRR=(0,0)\n",
    "    if len(all_results)>0:\n",
    "        for result in all_results:\n",
    "            MAP+=result['AP']\n",
    "            MRR+=1/result['first_pos'] if result['first_pos']>0 else 0\n",
    "        MAP/=len(all_results)\n",
    "        MRR/=len(all_results)\n",
    "        print(\"eligible_br_count: \",len(all_results))\n",
    "    return (MAP,MRR,len(all_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>TFIDF</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormValue(x,maximum,minimum):\n",
    "    return 6*((x - minimum)/(maximum - minimum))\n",
    "\n",
    "def getLenScore(length):\n",
    "    return (math.exp(length) / (1 + math.exp(length)))\n",
    "\n",
    "def calulateLengthScore(source_codes_df):\n",
    "    \"\"\"\n",
    "    Receives: a list of sizes of codes and the index\n",
    "    Process: calculate a boost score for the specified index based on length of that code\n",
    "    Returns: length boosting score \n",
    "    \"\"\"\n",
    "    average_size=source_codes_df['size'].mean()\n",
    "    standard_deviation=source_codes_df['size'].std() \n",
    "    low=average_size-3*standard_deviation\n",
    "    high= average_size+3*standard_deviation\n",
    "    minimum=int(low) if low>0 else 0\n",
    "        \n",
    "    len_scores=[]\n",
    "    for i,eachLen in source_codes_df['size'].items():\n",
    "        score=0\n",
    "        nor=getNormValue(eachLen,high,minimum)\n",
    "        if eachLen!=0:\n",
    "            if eachLen>=low and eachLen<=high:\n",
    "                score=getLenScore(nor)\n",
    "            elif eachLen<low:\n",
    "                score=0.5\n",
    "            elif eachLen>high:\n",
    "                score = 1.0\n",
    "        len_scores.append(score)\n",
    "    source_codes_df.loc[:,'lengthScore']=len_scores\n",
    "\n",
    "    return source_codes_df\n",
    "    \n",
    "def inverse_doc_freq(idf,D):\n",
    "    return math.log(D/idf)\n",
    "\n",
    "def term_freq(tf_list):\n",
    "    return [(math.log(tf+1)) for tf in tf_list]\n",
    "\n",
    "def np_normalizer(arr):\n",
    "    \"\"\"\n",
    "    @Receives: a list of numbers\n",
    "    @Process: normalizes all the values and map them to range of [0,1]\n",
    "    @Returns: list of normalized numbers\n",
    "    \"\"\"\n",
    "    if len(arr)>0:\n",
    "        maximum=np.amax(arr)\n",
    "        minimum=np.amin(arr)\n",
    "        if maximum!=minimum:\n",
    "            return (arr-minimum)/(maximum-minimum)\n",
    "    return arr\n",
    "\n",
    "def normalizer(Dict):\n",
    "    \"\"\"\n",
    "    @Receives: a list of numbers\n",
    "    @Process: normalizes all the values and map them to range of [0,1]\n",
    "    @Returns: list of normalized numbers\n",
    "    \"\"\"\n",
    "    if len(Dict)>0:\n",
    "        maximum=max(Dict.items(), key=operator.itemgetter(1))[1]\n",
    "        minimum=min(Dict.items(), key=operator.itemgetter(1))[1]\n",
    "        for key,value in Dict.items():\n",
    "            if maximum!=minimum:\n",
    "                Dict[key]=(value-minimum)/(maximum-minimum)\n",
    "            else:\n",
    "                Dict[key]=1.0\n",
    "    return Dict\n",
    "    \n",
    "\n",
    "def TFIDF_transform(all_bugs_df,source_codes_df):\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(list(source_codes_df['code']))\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in list(source_codes_df['code'])]\n",
    "    tfidf_weights = models.TfidfModel(corpus,wlocal=term_freq,wglobal=inverse_doc_freq,normalize=False)\n",
    "    source_codes_df.loc[:,'tfidf_vector']=tfidf_weights[corpus]\n",
    "    all_bugs_df['tfidf_vector']=all_bugs_df.text.apply(lambda x: tfidf_weights[dictionary.doc2bow(x)])\n",
    "    return (all_bugs_df,source_codes_df,len(dictionary))\n",
    "\n",
    "def cos_matrix_multiplication(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using matrix vector multiplication.\n",
    "    \"\"\"\n",
    "    dotted = matrix.dot(vector)\n",
    "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main BugLocalization class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BugLocalizer:\n",
    "\n",
    "    all_projects_source_codes=pd.DataFrame([])\n",
    "    all_projects_bugreports=pd.DataFrame([])\n",
    "    dataFolder=\"\"\n",
    "    \n",
    "    def __init__(self,project,result_path):\n",
    "        self.project=project\n",
    "        self.resultPath=result_path\n",
    "            \n",
    "    def execute(self):\n",
    "        print(\"\\t ****** Localizing Bugs for project: {} ******\".format(self.project))\n",
    "        self.loadBugCurpus()\n",
    "        self.loadSourceFiles()\n",
    "        self.localize()\n",
    "        self.evaluate()\n",
    "        self.to_csv()\n",
    "        self.write_result()\n",
    "        \n",
    "    def loadEverything():\n",
    "        if BugLocalizer.all_projects_bugreports.empty:\n",
    "            bugReportFile=os.path.join(BugLocalizer.dataFolder,'allBugReports.pickle')\n",
    "            if not os.path.isfile(bugReportFile):\n",
    "                print(\"The bug reports file (allBugReports.pickle) does not exist. please run the step0 first\")\n",
    "            else: \n",
    "                BugLocalizer.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "                print(\"*** All Bug Reports are Loaded. ***\")\n",
    "\n",
    "        if BugLocalizer.all_projects_source_codes.empty:\n",
    "            sourceCodeFile=os.path.join(BugLocalizer.dataFolder,'allSourceCodes.pickle')\n",
    "            if not os.path.isfile(sourceCodeFile):\n",
    "                print(\"The source codes file (allSourceCodes.pickle) does not exist. please run the step0 first\")\n",
    "            else:\n",
    "                BugLocalizer.all_projects_source_codes=pd.read_pickle(sourceCodeFile)\n",
    "                print(\"*** All Source Codes are Loaded. ***\")\n",
    "    \n",
    "    def loadBugCurpus(self):\n",
    "        self.all_bugs_df=BugLocalizer.all_projects_bugreports.loc[BugLocalizer.all_projects_bugreports['project']==self.project,:]\n",
    "        \n",
    "    def loadSourceFiles(self):\n",
    "        self.source_codes_df=BugLocalizer.all_projects_source_codes.loc[BugLocalizer.all_projects_source_codes['project']==self.project,:]\n",
    "    \n",
    "    def localize(self):\n",
    "        \n",
    "        print(\"Localizing Now ...\")\n",
    "        self.source_codes_df=calulateLengthScore(self.source_codes_df)\n",
    "        dictionary = gensim.corpora.Dictionary(list(self.source_codes_df['code']))\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in list(self.source_codes_df['code'])]\n",
    "        tfidf_weights = models.TfidfModel(corpus,wlocal=term_freq,wglobal=inverse_doc_freq,normalize=False)\n",
    "        self.source_codes_df.loc[:,'tfidf_vector']=tfidf_weights[corpus]\n",
    "        self.all_bugs_df.loc[:,'tfidf_vector']=self.all_bugs_df.text.apply(lambda x: tfidf_weights[dictionary.doc2bow(x)])\n",
    "        scores=[]\n",
    "        tfidf_index = similarities.SparseMatrixSimilarity(list(self.source_codes_df.tfidf_vector),num_features=len(dictionary))\n",
    "        for i, br in tqdm_notebook(self.all_bugs_df.iterrows()):\n",
    "            try:            \n",
    "                direct_tfidf_similarities=tfidf_index[br.tfidf_vector]\n",
    "                direct_tfidf_similarities=np_normalizer(direct_tfidf_similarities)\n",
    "                sourceCodeScores={self.source_codes_df.iloc[j].filename: (direct_tfidf_similarities[j])*self.source_codes_df.iloc[j].lengthScore \n",
    "                                                  for j in range(len(self.source_codes_df))}\n",
    "                scores.append({file:score for file,score in sorted(sourceCodeScores.items(),key=lambda tup: tup[1],reverse=True)})\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(traceback.format_exc())\n",
    "                scores.append({})                 \n",
    "                \n",
    "        self.all_bugs_df.loc[:,'total_score']=scores\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.result=evaluate(self.all_bugs_df,self.source_codes_df)\n",
    "        print(\"Result/\"+self.project+\":\\n\\t\",'*'*4,\" MAP: \",self.result[0],'*'*4,'\\n\\t','*'*4,\" MRR: \",self.result[1],'*'*4,\"\\n\",\"-\"*100)\n",
    "\n",
    "    def to_csv(self):\n",
    "        BugReports_path=os.path.join(self.resultPath,'BugReports') \n",
    "        SourceFiles_path=os.path.join(self.resultPath,'SourceFiles')\n",
    "        if not os.path.exists(BugReports_path):\n",
    "            os.makedirs(BugReports_path)\n",
    "        if not os.path.exists(SourceFiles_path):\n",
    "            os.makedirs(SourceFiles_path)\n",
    "        result_Bug_file=os.path.join(BugReports_path,self.project+\"_BugReports.csv\")\n",
    "        result_source_file=os.path.join(SourceFiles_path,self.project+\"_SourceFiles.csv\")\n",
    "        self.all_bugs_df.to_csv(result_Bug_file)\n",
    "        if len(self.all_bugs_df)<300:\n",
    "            self.source_codes_df.to_csv(result_source_file)\n",
    "        \n",
    "    def write_result(self):\n",
    "        group_result=open(os.path.join(self.resultPath,\"results_{}.csv\".format(self.project)),'w')\n",
    "        group_result.write(\"project , MAP , MRR , #ofBugReports\\n\")\n",
    "        group_result.write(project+','+str(self.result[0])+','+str(self.result[1])+','+str(self.result[2])+\"\\n\")\n",
    "        group_result.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def folder_structure(run_name):\n",
    "    result_path=os.path.join(os.getcwd(),\"Result\",run_name,strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    return result_path\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    run_name='5-Localtfidf'\n",
    "    result_path=folder_structure(run_name)\n",
    "    BugLocalizer.dataFolder=os.path.join(os.getcwd(),'Data')\n",
    "    if BugLocalizer.all_projects_bugreports.empty or BugLocalizer.all_projects_source_codes.empty:\n",
    "        BugLocalizer.loadEverything()\n",
    "    \n",
    "    all_projects=set(BugLocalizer.all_projects_bugreports.project)\n",
    "\n",
    "    for project in all_projects:\n",
    "        core=BugLocalizer(project=project,result_path=result_path)\n",
    "        core.execute()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Result</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method='5-Localtfidf'\n",
    "runNumber=\"run1\"\n",
    "all_results_csv=[os.path.join(os.getcwd(),\"Result\",method,runNumber,folder) \n",
    "                 for folder in listdir(os.path.join(os.getcwd(),\"Result\",method,runNumber)) if '.csv' in folder]\n",
    "results_df=pd.DataFrame([])\n",
    "for result_csv in all_results_csv:\n",
    "    res=pd.read_csv(result_csv,index_col=[0],header=0)\n",
    "    results_df=results_df.append(res)\n",
    "\n",
    "project_size_df=pd.read_csv('project_size.csv',index_col=[0],header=0)\n",
    "results_df=pd.merge(results_df, project_size_df,\n",
    "                                      left_index=True,\n",
    "                                      right_index=True)\n",
    "results_df=results_df.reset_index()\n",
    "results_df=results_df.set_index(' #ofBugReports')\n",
    "\n",
    "results_df.to_csv(os.path.join(os.getcwd(),\"Result\",method,'result.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
